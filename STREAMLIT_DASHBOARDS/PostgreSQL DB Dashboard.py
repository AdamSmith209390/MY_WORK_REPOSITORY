# ==================================================================================================
# HIGH-LEVEL OVERVIEW
# ==================================================================================================
#
# Project: BEELINK DATABASE DASHBOARD
# Description: A Streamlit web application to visualize the database inspection report
#              generated by the `db_inspection_csv.py` script. It provides an interactive
#              dashboard with key metrics, charts, and a filterable data table.
#
# How to run:
# 1. Make sure you have streamlit, pandas, and plotly installed:
#    pip install streamlit pandas plotly
# 2. Save this script as a Python file (e.g., `app.py`).
# 3. Run from your terminal in the same directory:
#    streamlit run app.py
#
# ==================================================================================================

import streamlit as st
import pandas as pd
import plotly.express as px
import os
import glob

# ==================================================================================================
# CONFIGURATION
# ==================================================================================================

# --- Page Configuration ---
# This should be the first Streamlit command in your script.
st.set_page_config(
    page_title="PostgreSQL DB Dashboard",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Path to the report directory ---
# This must match the `output_directory` in your inspection script.
REPORT_DIRECTORY = r"K:\AB_ALL_PYTHON\6_OUTPUT\OTHER\POSTGRE_SQL_DETAILS"

# ==================================================================================================
# HELPER FUNCTIONS
# ==================================================================================================

@st.cache_data
def load_data(file_path):
    """
    Loads the CSV data from the given file path into a pandas DataFrame.
    Uses Streamlit's caching to avoid reloading data on every interaction.
    """
    try:
        df = pd.read_csv(file_path)
        return df
    except FileNotFoundError:
        st.error(f"Error: The file was not found at `{file_path}`. Please ensure the inspection script has run and the path is correct.")
        return None
    except Exception as e:
        st.error(f"An error occurred while loading the CSV file: {e}")
        return None

def find_latest_report(directory):
    """
    Finds the most recently created CSV file in the specified directory.
    """
    try:
        list_of_files = glob.glob(os.path.join(directory, 'db_inspection_report_*.csv'))
        if not list_of_files:
            return None
        latest_file = max(list_of_files, key=os.path.getctime)
        return latest_file
    except Exception as e:
        st.warning(f"Could not automatically find the latest report: {e}")
        return None

# ==================================================================================================
# MAIN APPLICATION
# ==================================================================================================

def main():
    """
    The main function that builds and runs the Streamlit application.
    """
    # --- Title and Header ---
    st.title("üìä PostgreSQL Database Inspection Dashboard")
    st.markdown("An interactive overview of your database schema, tables, and data types.")

    # --- Sidebar for File Selection ---
    st.sidebar.title("üìÅ Data Source")
    
    # Attempt to find the latest report automatically
    latest_report_path = find_latest_report(REPORT_DIRECTORY)
    
    uploaded_file = st.sidebar.file_uploader(
        "Or upload a report CSV",
        type=['csv']
    )
    
    # Determine which file to load
    data_source = None
    if uploaded_file is not None:
        data_source = uploaded_file
        st.sidebar.success("Successfully loaded uploaded file.")
    elif latest_report_path:
        data_source = latest_report_path
        st.sidebar.info(f"Automatically loaded latest report:\n`{os.path.basename(latest_report_path)}`")
    else:
        st.warning(f"No inspection reports found in the directory: `{REPORT_DIRECTORY}`. Please run the inspection script or upload a file.")
        st.stop() # Stop execution if no data is available

    # --- Load and Process Data ---
    df = load_data(data_source)
    if df is None:
        st.stop() # Stop if data loading failed

    # Create a dataframe with unique tables for table-level stats
    table_df = df.drop_duplicates(subset=['table_name']).reset_index(drop=True)

    # --- Sidebar Filters ---
    st.sidebar.title("üîç Filters")
    selected_tables = st.sidebar.multiselect(
        "Select Tables to Display",
        options=table_df['table_name'].unique(),
        default=table_df['table_name'].unique()
    )

    if not selected_tables:
        st.warning("Please select at least one table to see the visualizations.")
        st.stop()

    # Filter the main dataframe based on selection
    df_filtered = df[df['table_name'].isin(selected_tables)]
    table_df_filtered = table_df[table_df['table_name'].isin(selected_tables)]

    # --- Key Metrics ---
    st.header("üìà Key Metrics")
    total_size_gb = table_df_filtered['table_size_bytes'].sum() / (1024**3)
    total_columns = len(df_filtered)
    total_tables = len(table_df_filtered)

    col1, col2, col3 = st.columns(3)
    col1.metric("Total Database Size (GB)", f"{total_size_gb:.2f}")
    col2.metric("Number of Tables", f"{total_tables}")
    col3.metric("Total Number of Columns", f"{total_columns}")

    st.markdown("---")

    # --- Visualizations ---
    st.header("üé® Visualizations")
    
    col_viz1, col_viz2 = st.columns(2)

    with col_viz1:
        # Treemap for Table Sizes
        st.subheader("Table Size Distribution")
        fig_treemap = px.treemap(
            table_df_filtered,
            path=[px.Constant("all"), 'table_name'],
            values='table_size_bytes',
            color='table_size_bytes',
            color_continuous_scale='Blues',
            hover_data={'table_size_pretty': True, 'table_size_bytes': False}
        )
        fig_treemap.update_layout(margin=dict(t=30, l=10, r=10, b=10))
        fig_treemap.update_traces(textinfo="label+text+percent root")
        st.plotly_chart(fig_treemap, use_container_width=True)

    with col_viz2:
        # Bar chart for Row Counts
        st.subheader("Row Count per Table")
        fig_bar = px.bar(
            table_df_filtered.sort_values('row_count', ascending=True),
            x='row_count',
            y='table_name',
            orientation='h',
            text='row_count',
            color='row_count',
            color_continuous_scale='Cividis'
        )
        fig_bar.update_layout(
            yaxis_title="Table Name",
            xaxis_title="Number of Rows",
            margin=dict(t=30, l=10, r=10, b=10),
            coloraxis_showscale=False
        )
        st.plotly_chart(fig_bar, use_container_width=True)

    # Pie chart for Data Type Distribution
    st.subheader("Overall Data Type Distribution")
    data_type_counts = df_filtered['data_type'].value_counts().reset_index()
    data_type_counts.columns = ['data_type', 'count']
    fig_pie = px.pie(
        data_type_counts,
        names='data_type',
        values='count',
        title='Proportion of Data Types Across Selected Tables',
        hole=0.3
    )
    fig_pie.update_traces(textposition='inside', textinfo='percent+label')
    st.plotly_chart(fig_pie, use_container_width=True)

    st.markdown("---")

    # --- Raw Data Explorer ---
    st.header("üìÑ Raw Data Explorer")
    st.markdown("View the detailed column information for the selected tables.")
    st.dataframe(df_filtered, use_container_width=True)


if __name__ == "__main__":
    main()
